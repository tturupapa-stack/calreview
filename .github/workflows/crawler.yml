name: Crawler Schedule

on:
  schedule:
    # 매일 오전 9시, 오후 10시 (KST 기준, UTC로는 -9시간)
    # UTC 00:00 = KST 09:00
    # UTC 13:00 = KST 22:00
    - cron: '0 0,13 * * *'
  workflow_dispatch: # 수동 실행도 가능하도록

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        working-directory: ./crawler
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run crawler
        working-directory: ./crawler
        run: |
          python -m crawler.main
        env:
          # Supabase 환경 변수 (GitHub Secrets에 등록 필요)
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

      - name: Upload crawler output
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: crawler-output
          path: crawler/output/*.json
          retention-days: 7

      - name: Notify on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `[크롤러 오류] ${new Date().toISOString()}`,
              body: `크롤러 실행 중 오류가 발생했습니다.\n\n실행 시간: ${new Date().toISOString()}\n워크플로우: ${context.workflow}\n실행 ID: ${context.runId}\n\n로그를 확인해주세요.`,
              labels: ['bug', 'crawler']
            })

