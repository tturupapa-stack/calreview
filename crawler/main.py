import importlib
import json
import os
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timezone
from typing import Dict, List

from crawler.models import Campaign
from crawler.utils import logger, save_campaigns_to_supabase, get_existing_source_ids, _campaign_to_supabase_dict

# í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸ ëª¨ë“ˆ ëª©ë¡
# ë ˆë·°(revu)ëŠ” ëª©ë¡ ì—´ëŒ ì‹œ ë¡œê·¸ì¸ì´ í•„ìš”í•˜ë¯€ë¡œ í˜„ì¬ëŠ” ì œì™¸
# 
# âš ï¸ ë¦¬ìŠ¤í¬ ê´€ë¦¬: robots.txt ìœ„ë°˜ ì‚¬ì´íŠ¸ëŠ” í¬ë¡¤ë§ ì¤‘ë‹¨
# - reviewnote: /campaigns/ ê²½ë¡œ ê¸ˆì§€
# - reviewplace: /pr ê²½ë¡œ ê¸ˆì§€  
# - seoulouba: ì „ì²´ ê²½ë¡œ ê¸ˆì§€ (ë£¨íŠ¸ë§Œ í—ˆìš©)
# - modooexperience: /campaign.php ê²½ë¡œ ê¸ˆì§€
# 
# âœ… ë²•ì  ë¦¬ìŠ¤í¬ ê²€í†  ì™„ë£Œ ì‚¬ì´íŠ¸ (2025ë…„ 1ì›”)
# - stylec, modan, myinfluencer, chuble, real_review, dinodan: robots.txt ë° ì´ìš©ì•½ê´€ í™•ì¸ ì™„ë£Œ
# 
# âœ… í™œì„± ì‚¬ì´íŠ¸ ëª©ë¡
SITES = [
    # ğŸ”´ ì¤‘ë‹¨ëœ ì‚¬ì´íŠ¸ (robots.txt ìœ„ë°˜)
    # "reviewnote",  # robots.txt: /campaigns/ ê¸ˆì§€
    # "dinnerqueen",  # ì¤‘ë‹¨
    # "gangnam",  # ëŒ€ì²´ë¨
    # "reviewplace",  # robots.txt: /pr ê¸ˆì§€
    # "seoulouba",  # robots.txt: ì „ì²´ ê¸ˆì§€
    # "modooexperience",  # robots.txt: /campaign.php ê¸ˆì§€
    # "pavlovu",  # ëŒ€ì²´ë¨
    # "myinfluencer",  # ìš´ì˜ ì¤‘ë‹¨
    # âœ… í™œì„± ì‚¬ì´íŠ¸ (ë²•ì  ë¦¬ìŠ¤í¬ ê²€í†  ì™„ë£Œ)
    "stylec",      # ìŠ¤íƒ€ì¼ì”¨
    "modan",       # ëª¨ë‘ì˜ì²´í—˜ë‹¨
    "chuble",      # ì¸„ë¸”
    "dinodan",     # ë””ë…¸ë‹¨
    "real_review", # ë¦¬ì–¼ë¦¬ë·°
]


def run_crawler(site_name: str) -> List[Campaign]:
    """íŠ¹ì • ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ ì‹¤í–‰ í›„ Campaign ë¦¬ìŠ¤íŠ¸ ë°˜í™˜."""

    try:
        logger.info("[%s] í¬ë¡¤ë§ ì‹œì‘...", site_name)
        module = importlib.import_module(f"crawler.sites.{site_name}")
        if hasattr(module, "crawl"):
            campaigns = module.crawl()
            logger.info("[%s] í¬ë¡¤ë§ ì™„ë£Œ - %dê°œ ìˆ˜ì§‘", site_name, len(campaigns))
            return campaigns
        else:
            logger.error("[%s] crawl í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", site_name)
    except ImportError:
        logger.error("[%s] ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", site_name)
    except Exception as e:
        logger.error("[%s] í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: %s", site_name, e)

    return []


def _campaign_to_dict(c: Campaign) -> Dict:
    """Campaign dataclassë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ dictë¡œ ë³€í™˜."""

    return {
        "title": c.title,
        "url": c.url,
        "site_name": c.site_name,
        "category": c.category,
        "deadline": c.deadline,
        "location": c.location,
        "image_url": c.image_url,
        "channel": c.channel,
        "type": c.type,
        "review_deadline_days": c.review_deadline_days,
    }


def cleanup_expired_campaigns() -> None:
    """ë§ˆê°ëœ ìº í˜ì¸ì„ is_active=falseë¡œ ì—…ë°ì´íŠ¸."""
    try:
        from dotenv import load_dotenv
        from supabase import create_client
        
        load_dotenv()
        supabase_url = os.getenv("NEXT_PUBLIC_SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
        
        if not supabase_url or not supabase_key:
            logger.warning("Supabase í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ì–´ ë§ˆê° ìº í˜ì¸ ì •ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        supabase = create_client(supabase_url, supabase_key)
        today = datetime.now(timezone.utc).date().isoformat()
        
        # ë¦¬ë·°ë…¸íŠ¸ ìº í˜ì¸ ì¤‘ ë§ˆê°ëœ ê²ƒ ì¡°íšŒ
        response = supabase.table("campaigns")\
            .select("id")\
            .eq("source", "reviewnote")\
            .eq("is_active", True)\
            .lt("application_deadline", today)\
            .execute()
        
        expired_count = len(response.data) if response.data else 0
        
        if expired_count > 0:
            # ì¼ê´„ ì—…ë°ì´íŠ¸
            for campaign in response.data:
                supabase.table("campaigns")\
                    .update({"is_active": False})\
                    .eq("id", campaign["id"])\
                    .execute()
            
            logger.info("ë§ˆê°ëœ ìº í˜ì¸ %dê°œ ë¹„í™œì„±í™” ì™„ë£Œ", expired_count)
        else:
            logger.info("ë§ˆê°ëœ ìº í˜ì¸ ì—†ìŒ")
            
    except Exception as e:
        logger.warning("ë§ˆê° ìº í˜ì¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): %s", e)


def main(save_json: bool = True, mode: str = "auto") -> None:
    """
    ì „ì²´ í¬ë¡¤ëŸ¬ ì‹¤í–‰.

    `mode` ì˜µì…˜:
    - "auto": campaigns í…Œì´ë¸”ì´ ë¹„ì–´ìˆìœ¼ë©´ "full", ì•„ë‹ˆë©´ "incremental"
    - "full": ëª¨ë“  í˜ì´ì§€ë¥¼ í¬ë¡¤ë§
    - "incremental": ì´ë¯¸ Supabaseì— ì¡´ì¬í•˜ëŠ” ìº í˜ì¸ì€ ê±´ë„ˆëœ€
    """

    # auto ëª¨ë“œ: campaigns í…Œì´ë¸” ìƒíƒœì— ë”°ë¼ ìë™ ê²°ì •
    if mode == "auto":
        existing_ids = get_existing_source_ids()
        if len(existing_ids) == 0:
            mode = "full"
            logger.info("campaigns í…Œì´ë¸”ì´ ë¹„ì–´ìˆìŒ -> full ëª¨ë“œë¡œ ì‹¤í–‰")
        else:
            mode = "incremental"
            logger.info("campaigns í…Œì´ë¸”ì— %dê°œ ìº í˜ì¸ ì¡´ì¬ -> incremental ëª¨ë“œë¡œ ì‹¤í–‰", len(existing_ids))

    logger.info("=== ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ (mode=%s) ===", mode)
    
    # ë§ˆê°ëœ ìº í˜ì¸ ì •ë¦¬
    cleanup_expired_campaigns()
    
    all_campaigns: List[Campaign] = []

    for site in SITES:
        campaigns = run_crawler(site)
        all_campaigns.extend(campaigns)

    logger.info("ì „ì²´ ì‚¬ì´íŠ¸ í•©ê³„: %dê°œ ìº í˜ì¸ ìˆ˜ì§‘", len(all_campaigns))

    # ì°¨ë“± í¬ë¡¤ë§: ê¸°ì¡´ IDì™€ ê²¹ì¹˜ëŠ” ìº í˜ì¸ ì œê±°
    if mode == "incremental":
        existing_ids = get_existing_source_ids()
        before = len(all_campaigns)
        all_campaigns = [c for c in all_campaigns if (c.site_name, _campaign_to_supabase_dict(c)["source_id"]) not in existing_ids]
        logger.info("ì°¨ë“± í•„í„°ë§: %d -> %d (ìƒˆë¡œìš´ ìº í˜ì¸ %dê°œ)", before, len(all_campaigns), len(all_campaigns))

    # ë°°ì¹˜ ì²˜ë¦¬: ë¦¬ë·° ê¸°ê°„ì´ ì—†ëŠ” ìº í˜ì¸ë“¤ì˜ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
    # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ ì¦ê°€ (ë” ë¹ ë¥¸ ì²˜ë¦¬)
    all_campaigns = enrich_review_deadlines_batch(all_campaigns, max_workers=10)

    # Supabaseì— ì €ì¥
    if all_campaigns:
        try:
            save_campaigns_to_supabase(all_campaigns)
        except Exception as e:
            logger.error("Supabase ì €ì¥ ì‹¤íŒ¨: %s", e)
            logger.info("JSON ì €ì¥ì€ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")

    if save_json and all_campaigns:
        # output ë””ë ‰í„°ë¦¬ ìƒì„±
        output_dir = os.path.join(os.path.dirname(__file__), "output")
        os.makedirs(output_dir, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = os.path.join(output_dir, f"campaigns_{timestamp}.json")

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(
                [_campaign_to_dict(c) for c in all_campaigns],
                f,
                ensure_ascii=False,
                indent=2,
            )

        logger.info("í¬ë¡¤ë§ ê²°ê³¼ JSON ì €ì¥ ì™„ë£Œ: %s", output_path)

    logger.info("=== ì „ì²´ í¬ë¡¤ë§ ì¢…ë£Œ ===")


def enrich_review_deadlines_batch(campaigns: List[Campaign], max_workers: int = 5) -> List[Campaign]:
    """ë¦¬ë·° ê¸°ê°„ì´ ì—†ëŠ” ìº í˜ì¸ë“¤ì˜ ìƒì„¸ í˜ì´ì§€ë¥¼ ë°°ì¹˜ë¡œ í¬ë¡¤ë§í•˜ì—¬ ë¦¬ë·° ê¸°ê°„ ì •ë³´ë¥¼ ì¶”ê°€.
    
    Args:
        campaigns: ë¦¬ë·° ê¸°ê°„ ì •ë³´ë¥¼ ì¶”ê°€í•  ìº í˜ì¸ ë¦¬ìŠ¤íŠ¸
        max_workers: ë³‘ë ¬ ì²˜ë¦¬í•  ìµœëŒ€ ì›Œì»¤ ìˆ˜
    
    Returns:
        ë¦¬ë·° ê¸°ê°„ ì •ë³´ê°€ ì¶”ê°€ëœ ìº í˜ì¸ ë¦¬ìŠ¤íŠ¸
    """
    from crawler.utils_detail import extract_detail_info
    
    # ë¦¬ë·° ê¸°ê°„ì´ ì—†ëŠ” ìº í˜ì¸ë§Œ í•„í„°ë§
    campaigns_to_enrich = [c for c in campaigns if c.review_deadline_days is None]
    
    if not campaigns_to_enrich:
        logger.info("ë¦¬ë·° ê¸°ê°„ ì •ë³´ê°€ í•„ìš”í•œ ìº í˜ì¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return campaigns
    
    logger.info("ë¦¬ë·° ê¸°ê°„ ì •ë³´ ì¶”ê°€ë¥¼ ìœ„í•´ %dê°œ ìº í˜ì¸ì˜ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘...", len(campaigns_to_enrich))
    
    def enrich_single_campaign(campaign: Campaign) -> tuple[Campaign, bool]:
        """ë‹¨ì¼ ìº í˜ì¸ì˜ ë¦¬ë·° ê¸°ê°„ ì •ë³´ë¥¼ ì¶”ê°€."""
        try:
            info = extract_detail_info(campaign.url, campaign.site_name)
            review_deadline_days = info.get("review_deadline_days")
            
            if review_deadline_days:
                # Campaign ê°ì²´ëŠ” ë¶ˆë³€(immutable)ì´ë¯€ë¡œ ìƒˆ ê°ì²´ ìƒì„±
                return Campaign(
                    title=campaign.title,
                    url=campaign.url,
                    site_name=campaign.site_name,
                    category=campaign.category,
                    deadline=campaign.deadline,
                    location=campaign.location,
                    image_url=campaign.image_url,
                    channel=campaign.channel,
                    type=campaign.type,
                    review_deadline_days=review_deadline_days,
                ), True
            else:
                return campaign, False
        except Exception as e:
            logger.warning("[%s] ë¦¬ë·° ê¸°ê°„ ì •ë³´ ì¶”ê°€ ì‹¤íŒ¨: %s (URL: %s)", campaign.site_name, e, campaign.url)
            return campaign, False
    
    # ìº í˜ì¸ì„ URLë¡œ ì¸ë±ì‹±í•˜ì—¬ ë¹ ë¥¸ ì¡°íšŒ ê°€ëŠ¥í•˜ë„ë¡
    campaign_dict = {c.url: c for c in campaigns}
    enriched_count = 0
    
    # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_campaign = {
            executor.submit(enrich_single_campaign, c): c 
            for c in campaigns_to_enrich
        }
        
        for future in as_completed(future_to_campaign):
            campaign = future_to_campaign[future]
            try:
                enriched_campaign, success = future.result()
                campaign_dict[enriched_campaign.url] = enriched_campaign
                if success:
                    enriched_count += 1
            except Exception as e:
                logger.warning("[%s] ë¦¬ë·° ê¸°ê°„ ì •ë³´ ì¶”ê°€ ì¤‘ ì˜ˆì™¸ ë°œìƒ: %s", campaign.site_name, e)
    
    logger.info("ë¦¬ë·° ê¸°ê°„ ì •ë³´ ì¶”ê°€ ì™„ë£Œ: %d/%dê°œ ì„±ê³µ", enriched_count, len(campaigns_to_enrich))
    
    # ì›ë˜ ìˆœì„œ ìœ ì§€í•˜ë©´ì„œ ì—…ë°ì´íŠ¸ëœ ìº í˜ì¸ ë°˜í™˜
    return [campaign_dict.get(c.url, c) for c in campaigns]


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run crawler")
    parser.add_argument("--mode", choices=["auto", "full", "incremental"], default="auto",
                        help="auto: auto-detect (full if empty, incremental if not); full: crawl all pages; incremental: skip already stored campaigns")
    args = parser.parse_args()
    main(save_json=True, mode=args.mode)
